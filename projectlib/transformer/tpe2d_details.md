The paper doesn't actually build a â€œ2D embedding vectorâ€ in the usual sense. Instead they reuse vanilla 1D RoPE several times on different 1D traversals of the table, and then let each attention head route itself to the traversal it finds most useful. 

Step by step implementation written (mostly) by ChatGPT and verified by me.

1. Recap: what RoPE is doing (Â§3.1)

Standard (1D) RoPE works like this: for a sequence of tokens 
ğ‘¥
1
,
â€¦
,
ğ‘¥
ğ‘€
x
1
	â€‹

,â€¦,x
M
	â€‹

, an attention head 
â„
h has queries, keys, values

ğ‘
ğ‘š
â„
,
ğ‘˜
ğ‘›
â„
,
ğ‘£
ğ‘›
â„
âˆˆ
ğ‘…
ğ‘‘
.
q
m
h
	â€‹

,k
n
h
	â€‹

,v
n
h
	â€‹

âˆˆR
d
.

Causal self-attention output for head 
â„
h and token 
ğ‘š
m is

ğ‘œ
ğ‘š
â„
=
âˆ‘
ğ‘›
â‰¤
ğ‘š
ğ‘
ğ‘š
,
ğ‘›
â„
â€‰
ğ‘£
ğ‘›
â„
,
o
m
h
	â€‹

=
nâ‰¤m
âˆ‘
	â€‹

a
m,n
h
	â€‹

v
n
h
	â€‹

,

with

ğ‘
ğ‘š
,
ğ‘›
â„
=
exp
â¡
(
ğ‘“
(
ğ‘
ğ‘š
â„
,
ğ‘˜
ğ‘›
â„
)
)
âˆ‘
ğ‘—
â‰¤
ğ‘š
exp
â¡
(
ğ‘“
(
ğ‘
ğ‘š
â„
,
ğ‘˜
ğ‘—
â„
)
)
.
a
m,n
h
	â€‹

=
âˆ‘
jâ‰¤m
	â€‹

exp(f(q
m
h
	â€‹

,k
j
h
	â€‹

))
exp(f(q
m
h
	â€‹

,k
n
h
	â€‹

))
	â€‹

.

RoPE defines

ğ‘“
(
ğ‘
ğ‘š
â„
,
ğ‘˜
ğ‘›
â„
)
=
(
ğ‘
ğ‘š
â„
)
âŠ¤
ğ‘…
ğ‘
,
ğ‘‘
ğ‘›
âˆ’
ğ‘š
ğ‘˜
ğ‘›
â„
,
f(q
m
h
	â€‹

,k
n
h
	â€‹

)=(q
m
h
	â€‹

)
âŠ¤
R
b,d
nâˆ’m
	â€‹

k
n
h
	â€‹

,

where 
ğ‘…
ğ‘
,
ğ‘‘
ğ‘š
âˆˆ
ğ‘…
ğ‘‘
Ã—
ğ‘‘
R
b,d
m
	â€‹

âˆˆR
dÃ—d
 is a block-diagonal rotation matrix; each 2-dim subspace is rotated by angle 
ğ‘š
ğœƒ
ğ‘
,
ğ‘‘
,
ğ‘–
mÎ¸
b,d,i
	â€‹

 (with different frequencies per block).

Key property: the attention score depends only on the relative offset 
ğ‘›
âˆ’
ğ‘š
nâˆ’m, but you can implement it by rotating queries/keys according to absolute position indices.

So in plain 1D RoPE you assign a single scalar position index 
ğ‘š
m per token, and then use that index to build the rotation.

2. Their core idea: represent â€œ2D positionâ€ as several 1D positions

The paperâ€™s setting: input is

a question 
ğ‘„
Q,

a table 
ğ‘‡
T,

a text instruction â€œAnswer:â€.

They concatenate those into a single token sequence

ğ‘‹
=
(
ğ‘¥
1
,
ğ‘¥
2
,
â€¦
,
ğ‘¥
ğ‘€
)
.
X=(x
1
	â€‹

,x
2
	â€‹

,â€¦,x
M
	â€‹

).

Then instead of a single position index per token, they assign a vector of indices

ğ‘ƒ
=
(
ğ‘
1
,
â€¦
,
ğ‘
ğ‘€
)
,
ğ‘
ğ‘š
=
(
ğ‘
ğ‘š
,
1
,
â€¦
,
ğ‘
ğ‘š
,
ğ½
)
,
P=(p
1
	â€‹

,â€¦,p
M
	â€‹

),p
m
	â€‹

=(p
m,1
	â€‹

,â€¦,p
m,J
	â€‹

),

where

ğ‘
ğ‘š
,
ğ‘—
 is the position index of token 
ğ‘¥
ğ‘š
 under permutation order 
ğ‘—
.
p
m,j
	â€‹

 is the position index of token x
m
	â€‹

 under permutation order j.

Each permutation order 
ğ‘—
j corresponds to a traversal mode over the table (e.g. row-wise, column-wise, diagonal, Hilbert curve,â€¦).

In this paper they actually use only two: row-wise and column-wise traversals (
ğ½
=
2
J=2).

So â€œ2D positional encodingâ€ = give each token multiple 1D RoPE indices, each reflecting one way of walking through the 2D table.

Important design points from Â§4.3:

Table tokens: for each table cell, you traverse the table in different orders (row-wise, column-wise, etc.) and assign indices accordingly.
â€“ Row-wise: scan rows leftâ†’right, topâ†’bottom.
â€“ Column-wise: scan columns topâ†’bottom, leftâ†’right.
â€“ Tokens inside the same cell keep the same relative order in all traversals.

Text tokens (question + â€œAnswer:â€ + any plain text): they simply get the same monotonically increasing index in every permutation order; i.e. 
ğ‘
ğ‘š
,
1
=
ğ‘
ğ‘š
,
2
=
â€¦
p
m,1
	â€‹

=p
m,2
	â€‹

=â€¦. So pure text behaves exactly like a normal 1D RoPE LLM.

Generated answer tokens: during generation, they continue incrementing the position indices for all permutation orders in lockstep, again matching vanilla RoPE for the answer span.

So the whole 2D-ness is in how you assign these index vectors 
ğ‘
ğ‘š
p
m
	â€‹

.

3. 2D-TPE attention: mixture of several RoPE-based attentions

Given:

token sequence 
ğ‘‹
X,

per-token index vectors 
ğ‘
ğ‘š
=
(
ğ‘
ğ‘š
,
1
,
.
.
.
,
ğ‘
ğ‘š
,
ğ½
)
p
m
	â€‹

=(p
m,1
	â€‹

,...,p
m,J
	â€‹

),

they modify each self-attention layer like this.

3.1 Per-head mixture over permutation orders

For a head 
â„
h and token 
ğ‘¥
ğ‘š
x
m
	â€‹

, instead of one attention output they compute one attention output per permutation order, then mix them:

	
ğ‘œ
ğ‘š
â„
=
âˆ‘
ğ‘—
=
1
ğ½
ğ‘Ÿ
ğ‘š
,
ğ‘—
â„
â€‰
ğ‘œ
ğ‘š
,
ğ‘—
â„
.
		
(7)
o
m
h
	â€‹

=
j=1
âˆ‘
J
	â€‹

r
m,j
h
	â€‹

o
m,j
h
	â€‹

.
(7)

ğ‘œ
ğ‘š
,
ğ‘—
â„
o
m,j
h
	â€‹

 = attention output for head 
â„
h, token 
ğ‘š
m, using order 
ğ‘—
j.

ğ‘Ÿ
ğ‘š
,
ğ‘—
â„
r
m,j
h
	â€‹

 = routing weight saying how much this head, for this token, trusts permutation 
ğ‘—
j.

The routing weights come from a small MLP router per head:

	
ğ‘Ÿ
ğ‘š
,
ğ‘—
â„
=
Softmax
(
MLP
(
â„
ğ‘š
â„
)
)
ğ‘—
,
		
(8)
r
m,j
h
	â€‹

=Softmax(MLP(h
m
h
	â€‹

))
j
	â€‹

,
(8)

where 
â„
ğ‘š
h
m
	â€‹

 is the hidden state at that layer and 
â„
ğ‘š
â„
h
m
h
	â€‹

 is the slice for head 
â„
h.

Router MLP is LLaMA-style gated FFN:

MLP
(
â„
ğ‘š
â„
)
=
ğ‘Š
down
(
SiLU
(
ğ‘Š
up
â€‰
â„
ğ‘š
â„
)
âŠ™
(
ğ‘Š
gate
â€‰
â„
ğ‘š
â„
)
)
,
MLP(h
m
h
	â€‹

)=W
down
	â€‹

(SiLU(W
up
	â€‹

h
m
h
	â€‹

)âŠ™(W
gate
	â€‹

h
m
h
	â€‹

)),

with

ğ‘Š
up
âˆˆ
ğ‘…
4
ğ‘‘
Ã—
ğ‘‘
W
up
	â€‹

âˆˆR
4dÃ—d
,

ğ‘Š
gate
âˆˆ
ğ‘…
4
ğ‘‘
Ã—
ğ‘‘
W
gate
	â€‹

âˆˆR
4dÃ—d
,

ğ‘Š
down
âˆˆ
ğ‘…
ğ½
Ã—
4
ğ‘‘
W
down
	â€‹

âˆˆR
JÃ—4d
.

So per (head, token) you get a length-
ğ½
J logit vector, softmax it â†’ routing distribution.

Intuition: each head + token decides â€œfor this query, do I want to look at the world in row-wise mode, column-wise mode, â€¦?â€.

3.2 Attention for a fixed permutation order 
ğ‘—
j

For a given order 
ğ‘—
j, you just do standard causal attention with 1D RoPE, but using position indices 
ğ‘
ğ‘š
,
ğ‘—
p
m,j
	â€‹

 instead of plain sequence indices:

	
ğ‘œ
ğ‘š
,
ğ‘—
â„
=
âˆ‘
ğ‘
ğ‘›
,
ğ‘—
â‰¤
ğ‘
ğ‘š
,
ğ‘—
ğ‘
ğ‘š
,
ğ‘›
,
ğ‘—
â„
â€‰
ğ‘£
ğ‘›
â„
,
		
(10)
o
m,j
h
	â€‹

=
p
n,j
	â€‹

â‰¤p
m,j
	â€‹

âˆ‘
	â€‹

a
m,n,j
h
	â€‹

v
n
h
	â€‹

,
(10)

with

	
ğ‘
ğ‘š
,
ğ‘›
,
ğ‘—
â„
=
exp
â¡
(
(
ğ‘
ğ‘š
â„
)
âŠ¤
ğ‘…
ğ‘
,
ğ‘‘
ğ‘
ğ‘›
,
ğ‘—
âˆ’
ğ‘
ğ‘š
,
ğ‘—
ğ‘˜
ğ‘›
â„
)
âˆ‘
ğ‘
ğ‘–
,
ğ‘—
â‰¤
ğ‘
ğ‘š
,
ğ‘—
exp
â¡
(
(
ğ‘
ğ‘š
â„
)
âŠ¤
ğ‘…
ğ‘
,
ğ‘‘
ğ‘
ğ‘–
,
ğ‘—
âˆ’
ğ‘
ğ‘š
,
ğ‘—
ğ‘˜
ğ‘–
â„
)
.
		
(11)
a
m,n,j
h
	â€‹

=
âˆ‘
p
i,j
	â€‹

â‰¤p
m,j
	â€‹

	â€‹

exp((q
m
h
	â€‹

)
âŠ¤
R
b,d
p
i,j
	â€‹

âˆ’p
m,j
	â€‹

	â€‹

k
i
h
	â€‹

)
exp((q
m
h
	â€‹

)
âŠ¤
R
b,d
p
n,j
	â€‹

âˆ’p
m,j
	â€‹

	â€‹

k
n
h
	â€‹

)
	â€‹

.
(11)

Key points:

Same projection matrices to get 
ğ‘
â„
,
ğ‘˜
â„
,
ğ‘£
â„
q
h
,k
h
,v
h
: we donâ€™t change Q/K/V definitions, only the RoPE and mask.

Causal mask in order 
ğ‘—
j: token with index 
ğ‘
ğ‘š
,
ğ‘—
p
m,j
	â€‹

 can attend only to tokens with index 
ğ‘
ğ‘›
,
ğ‘—
â‰¤
ğ‘
ğ‘š
,
ğ‘—
p
n,j
	â€‹

â‰¤p
m,j
	â€‹

. So causality is defined along that permutationâ€™s linearization.

Because the sequence order of 
ğ‘‹
X itself is â€œinessentialâ€ (they say this explicitly), they re-rank Q/K/V by increasing 
ğ‘
ğ‘š
,
ğ‘—
p
m,j
	â€‹

 before computing attention, so the causal mask is the usual lower-triangular mask in that order. After attention, you map outputs back to the original token order.

So for each permutation order:

Sort tokens by 
ğ‘
â‹…
,
ğ‘—
p
â‹…,j
	â€‹

 (ascending).

Apply standard QKV attention with RoPE on that sorted sequence.

Undo the sort to align outputs with original token indices.

Call that 
ğ‘œ
ğ‘š
,
ğ‘—
â„
o
m,j
h
	â€‹

.

Then mix across 
ğ‘—
j using the router weights as in (7).

4. How the â€œ2Dâ€ indices are constructed (candidate permutation orders, Â§4.3)

They discuss several possibilities: row-wise, column-wise, diagonal, Hilbert curve, Z-order curve, etc. Any such traversal induces a permutation of the tokens and hence a scalar index for each token.

In this paper, they fix 
ğ½
=
2
J=2:

Order 1: row-wise traversal from top-left to bottom-right.

Order 2: column-wise traversal from top-left to bottom-right.

For each traversal 
ğ‘—
j:

Walk the table, cell by cell.

Within each cell, traverse WordPiece/BPE tokens in reading order, assigning indices 
ğ‘
ğ‘š
,
ğ‘—
p
m,j
	â€‹

 sequentially.

Tokens in the same cell stay contiguous and in the same relative order for all permutations (only distances between cells change).

For text tokens (question and â€œAnswer:â€ plus any other plain text around the table): all permutations share the same incremental index sequence; so text-only attention is exactly like standard RoPE.

During generation, they â€œincrementally assign position indices to generated tokensâ€ in all permutation orders, again mimicking standard 1D RoPE for the answer span.

So effectively:

Table cells â†’ different 
ğ‘
ğ‘š
,
ğ‘—
p
m,j
	â€‹

 for different ways of linearizing the 2D grid.

Question/Answer â†’ same 
ğ‘
ğ‘š
,
ğ‘—
p
m,j
	â€‹

 across all 
ğ‘—
j, so 2D-TPE degenerates to 1D RoPE there.

5. Training: making heads specialize to traversal modes (Â§4.2)

The base loss is standard language modeling loss on the answer:

	
ğ¿
nll
=
âˆ’
log
â¡
ğ‘ƒ
(
ğ´
âˆ£
ğ‘„
,
ğ‘‡
)
.
		
(12)
L
nll
	â€‹

=âˆ’logP(Aâˆ£Q,T).
(12)

But if you only use that, router distributions 
ğ‘Ÿ
ğ‘š
,
ğ‘—
â„
r
m,j
h
	â€‹

 could stay diffuse (all permutation orders mixed equally), which makes the model harder to interpret and possibly inefficient.

So they add an entropy regularization term on the router distributions:

ğ¸
ğ‘š
â„
=
âˆ’
âˆ‘
ğ‘—
=
1
ğ½
ğ‘Ÿ
ğ‘š
,
ğ‘—
â„
log
â¡
ğ‘Ÿ
ğ‘š
,
ğ‘—
â„
,
E
m
h
	â€‹

=âˆ’
j=1
âˆ‘
J
	â€‹

r
m,j
h
	â€‹

logr
m,j
h
	â€‹

,
	
ğ¿
ent
=
1
ğ‘€
ğ»
âˆ‘
ğ‘š
=
1
ğ‘€
âˆ‘
â„
=
1
ğ»
ğ¸
ğ‘š
â„
.
		
(13â€“14)
L
ent
	â€‹

=
MH
1
	â€‹

m=1
âˆ‘
M
	â€‹

h=1
âˆ‘
H
	â€‹

E
m
h
	â€‹

.
(13â€“14)

Total loss:

	
ğ¿
=
ğ¿
nll
+
ğœ†
â€‰
ğ¿
ent
.
		
(15)
L=L
nll
	â€‹

+Î»L
ent
	â€‹

.
(15)

They minimize 
ğ¿
ent
L
ent
	â€‹

, thus pushing 
ğ‘Ÿ
ğ‘š
,
â‹…
â„
r
m,â‹…
h
	â€‹

 towards low-entropy distributions â€” ideally each head+token strongly prefers one permutation order instead of blending them.

Intuition: each head becomes something like â€œa row headâ€ or â€œa column headâ€ (or later â€œa diagonal headâ€, etc.), for specific regions/tokens.
